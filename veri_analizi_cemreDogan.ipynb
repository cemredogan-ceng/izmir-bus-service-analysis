{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b6f6d2-a471-4a67-a046-9b9b39136e9f",
   "metadata": {},
   "source": [
    "# Otobüs Hareket Saatleri ve Durakları Verisetleri Analizi\n",
    "\n",
    "Bu çalışmada İzmir’de 757 ve 619 numaralı otobüs hatları için saat–gün bazında sefer yoğunluğunu çıkardım. Hareket saatlerini okuyup TARIFE_ID’yi gün tipine çevirdim, gidiş/dönüş saatlerini tek kolonda birleştirip gün×saat kırılımında sefer sayılarını saydım ve ısı haritalarını ürettim. Duraklar verisinden iki hattın duraklarını harita üzerinde gösterdim ve Ulukent Aktarma Merkezi ile Bakırçay Üniversitesi için 300 m içinde durak olup olmadığını mesafe (haversine) hesabıyla doğruladım. Ek olarak, tüm hatlar için en çok sefer yapanlar sıralamasını çıkardım. (İleride K-Means ile hat profili kümeleme eklenebilir.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd35ce70-2ed1-422b-8427-7d8ce67f99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"  \n",
    "\n",
    "import os, random, numpy as np\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"  # Windows+MKL uyarısını bastırmak için\n",
    "random.seed(42); np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb9024-12f7-4718-9251-6aea6a8e94c3",
   "metadata": {},
   "source": [
    "### Veri Okuma ve Kurulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c87129-0abe-46ad-b8ab-15caeaf3c7f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\cemre\\\\Desktop\\\\eshot-otobus-hareketsaatleri.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m p_durak   \u001b[38;5;241m=\u001b[39m DESKTOP \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meshot-otobus-duraklari.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# CSV'leri okuyalım\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m hareket \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(p_hareket, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m duraklar \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(p_durak, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Tip ve hızlı kontrol yapalım\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\cemre\\\\Desktop\\\\eshot-otobus-hareketsaatleri.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Grafiklerin inline görünmesi\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dosyalarımızı tanımlayalım\n",
    "DESKTOP = Path.home() / \"Desktop\"\n",
    "p_hareket = DESKTOP / \"eshot-otobus-hareketsaatleri.csv\"\n",
    "p_durak   = DESKTOP / \"eshot-otobus-duraklari.csv\"\n",
    "\n",
    "# CSV'leri okuyalım\n",
    "hareket = pd.read_csv(p_hareket, sep=\";\", encoding=\"utf-8\")\n",
    "duraklar = pd.read_csv(p_durak, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "# Tip ve hızlı kontrol yapalım\n",
    "hareket[\"HAT_NO\"] = pd.to_numeric(hareket[\"HAT_NO\"], errors=\"coerce\")\n",
    "print(\"hareket:\", hareket.shape, \"| duraklar:\", duraklar.shape)\n",
    "display(hareket.head(3))\n",
    "display(duraklar.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b89259-4f71-4586-8c7e-53782491934d",
   "metadata": {},
   "source": [
    "### Seçilen Hatlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e199d1-5711-475b-a3bd-f1a164026609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARIFE_ID → Gün adı olarak gruplayalım\n",
    "tarife_map = {1: \"Hafta içi\", 2: \"Cumartesi\", 3: \"Pazar\"}\n",
    "hareket[\"GUN\"] = hareket[\"TARIFE_ID\"].map(tarife_map)\n",
    "\n",
    "# Gidiş/Dönüş saatlerini tek kolonda toplayalım \n",
    "long = pd.melt(\n",
    "    hareket,\n",
    "    id_vars=[\"HAT_NO\", \"TARIFE_ID\", \"GUN\"],\n",
    "    value_vars=[\"GIDIS_SAATI\", \"DONUS_SAATI\"],\n",
    "    var_name=\"YON\",\n",
    "    value_name=\"SAAT_STR\",\n",
    ")\n",
    "\n",
    "# Geçersiz saatleri atalım ve 0–23 saat değerini çıkar\n",
    "long = long[long[\"SAAT_STR\"].str.match(r\"^\\d{2}:\\d{2}$\", na=False)].copy()\n",
    "long[\"SAAT\"] = pd.to_datetime(long[\"SAAT_STR\"], format=\"%H:%M\").dt.hour\n",
    "\n",
    "\n",
    "# Okul için kullandığımız hatları seçmek istedim.Fakat 812 sonradan eklenen bir hat olduğu için verisetine dahil edilmemiş. Bu yüzden 619 ve 757 ile çalışmaya devam ediyorum.\n",
    "print(\"long:\", long.shape, \"| 757 var mı?\", (long[\"HAT_NO\"]==757).any(), \"| 619 var mı?\", (long[\"HAT_NO\"]==619).any(), \"| 812 var mı?\", (long[\"HAT_NO\"]==812).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82254d57-71ee-4db1-925a-5e6f229540a6",
   "metadata": {},
   "source": [
    "### Sefer Sıklığı Isı Haritaları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ae242-2a10-40b9-9fd5-bf026df37637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(df_long: pd.DataFrame, hat_no: int, save_dir=DESKTOP, vmin=0, vmax=None):\n",
    "    gun_sira = [\"Hafta içi\", \"Cumartesi\", \"Pazar\"]\n",
    "    df_hat = df_long[df_long[\"HAT_NO\"] == hat_no].copy()\n",
    "    assert not df_hat.empty, f\"HAT {hat_no}: veri bulunamadı.\"\n",
    "\n",
    "    pivot = pd.crosstab(df_hat[\"GUN\"], df_hat[\"SAAT\"]).reindex(gun_sira).fillna(0)\n",
    "    pivot = pivot.reindex(columns=list(range(24)), fill_value=0)  # 0–23 eksiksiz\n",
    "\n",
    "    # vmax dışarıdan verilmemişse bu hattın tavanını kullan\n",
    "    vmax = pivot.values.max() if vmax is None else vmax\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    im = ax.imshow(pivot.values, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"HAT {hat_no} - Saat–Gün Isı Haritası (Sefer Yoğunluğu)\")\n",
    "    ax.set_xlabel(\"Saat (0–23)\")\n",
    "    ax.set_ylabel(\"Gün\")\n",
    "    ax.set_yticks(range(len(gun_sira)))\n",
    "    ax.set_yticklabels(gun_sira)\n",
    "    fig.colorbar(im, ax=ax, label=\"Sefer sayısı\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_png = (save_dir / f\"heatmap_hat_{hat_no}.png\").as_posix()\n",
    "    fig.savefig(out_png, dpi=150)\n",
    "    print(\"Kaydedildi:\", out_png)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- ORTAK vmax HESABI ve ÇİZİM \n",
    "subset = long[ long[\"HAT_NO\"].isin([757, 619]) ]\n",
    "cnt = (subset.groupby([\"HAT_NO\",\"GUN\",\"SAAT\"]).size().rename(\"cnt\"))\n",
    "global_vmax = int(cnt.max()) if not cnt.empty else None\n",
    "\n",
    "\n",
    "\n",
    "for h in [757, 619]:\n",
    "    plot_heatmap(long, h, vmin=0, vmax=global_vmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3734b-bc13-4344-a244-3fc323b7565e",
   "metadata": {},
   "source": [
    "ISI HARİTALARI YORUMU\n",
    "\n",
    "757: Hafta içi 06–08’de keskin bir sabah zirvesi var; 08 sonrası gün boyu yüksek ve sürekli bir plato görülüyor. Hafta sonu belirgin şekilde seyrekleşiyor, Pazar en düşük.\n",
    "619: Hafta içi 06’dan sonra hızla yükselip gün boyunca yüksek kalıyor; akşama kadar hizmet güçlü. Hafta sonu düşüyor, Pazar en düşük.\n",
    "Karşılaştırma: İki hat da hafta içi daha sık; 757’de zirve daha keskin, 619’da yoğunluk daha dengeli ve gün boyu yayılmış. (Haritalar aynı renk ölçeğiyle çizildi, bu yüzden farklar doğrudan karşılaştırılabilir.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad87c4-8c1b-4245-be7b-e6739bfe9d5d",
   "metadata": {},
   "source": [
    "### Durakların Haritada Gösterimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543b619-e1a4-496d-a499-ffa2359cc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARİTA BLOĞU \n",
    "import pandas as pd, folium\n",
    "from pathlib import Path\n",
    "from IPython.display import display, IFrame\n",
    "\n",
    "assert 'duraklar' in globals(), \"Önce duraklar CSV'sini oku: duraklar = pd.read_csv(..., sep=';')\"\n",
    "\n",
    "def stops_for(hat_no: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = df[\"DURAKTAN_GECEN_HATLAR\"].astype(str)\n",
    "    def has_exact(cell):\n",
    "        raw = str(cell)\n",
    "        for sep in [\";\", \",\", \"/\", \"\\\\\", \"|\", \"-\", \" \"]:\n",
    "            raw = raw.replace(sep, \"|\")\n",
    "        toks = [t.strip() for t in raw.split(\"|\") if t.strip()]\n",
    "        return str(hat_no) in toks\n",
    "    mask = s.apply(has_exact) | s.str.contains(fr\"\\b{hat_no}\\b\", regex=True, na=False)\n",
    "    out = df[mask].copy()\n",
    "    out[\"ENLEM\"]  = pd.to_numeric(out[\"ENLEM\"], errors=\"coerce\")\n",
    "    out[\"BOYLAM\"] = pd.to_numeric(out[\"BOYLAM\"], errors=\"coerce\")\n",
    "    return out.dropna(subset=[\"ENLEM\",\"BOYLAM\"]).drop_duplicates(subset=[\"DURAK_ID\"]).reset_index(drop=True)\n",
    "\n",
    "# Veriler\n",
    "d757 = stops_for(757, duraklar)\n",
    "d619 = stops_for(619, duraklar)\n",
    "print(f\"Durak sayısı -> 757: {len(d757)} | 619: {len(d619)}\")\n",
    "\n",
    "# Merkez (fallback'lı)\n",
    "all_pts = pd.concat(\n",
    "    [d757[[\"ENLEM\",\"BOYLAM\"]], d619[[\"ENLEM\",\"BOYLAM\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "if not all_pts.empty and all_pts[\"ENLEM\"].notna().any() and all_pts[\"BOYLAM\"].notna().any():\n",
    "    center = [all_pts[\"ENLEM\"].mean(), all_pts[\"BOYLAM\"].mean()]\n",
    "else:\n",
    "    # İzmir genel fallback veya bildiğin bir nokta\n",
    "    center = [38.42, 27.14]  # İzmir merkez civarı\n",
    "    print(\"Uyarı: Merkez için yedek konum kullanıldı (veri boş olabilir).\")\n",
    "\n",
    "# Harita\n",
    "m = folium.Map(location=center, zoom_start=11, tiles=\"OpenStreetMap\")\n",
    "\n",
    "def add_layer(df, name, color):\n",
    "    fg = folium.FeatureGroup(name=name, show=True)\n",
    "    for _, r in df.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            [float(r[\"ENLEM\"]), float(r[\"BOYLAM\"])],\n",
    "            radius=3, color=color, fill=True, fill_color=color, fill_opacity=0.9,\n",
    "            tooltip=folium.Tooltip(f\"{r['DURAK_ADI']}\", sticky=True)\n",
    "        ).add_to(fg)\n",
    "    fg.add_to(m)\n",
    "\n",
    "add_layer(d757, f\"HAT 757 (n={len(d757)})\", \"#e74c3c\")\n",
    "add_layer(d619, f\"HAT 619 (n={len(d619)})\", \"#1f77b4\")\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "\n",
    "display(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb4280-76b8-4577-ab57-d98fb6ad163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilinen noktalardan 757 & 619 geçiyor mu? \n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "assert 'duraklar' in globals(), \"Önce duraklar CSV'sini oku: duraklar = pd.read_csv(..., sep=';')\"\n",
    "\n",
    "# 1) Bilinen noktalar ve eşik\n",
    "known_points = [\n",
    "    {\"name\": \"Ulukent Aktarma Merkezi\", \"lat\": 38.547269, \"lon\": 27.035109},\n",
    "    {\"name\": \"Bakırçay Üniversitesi\",   \"lat\": 38.58229,  \"lon\": 26.964101},\n",
    "]\n",
    "radius_m = 300\n",
    "hats = [757, 619]\n",
    "\n",
    "# 2) Hattan geçen durakları eksiksiz çekelim \n",
    "def stops_for(hat_no: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = df[\"DURAKTAN_GECEN_HATLAR\"].astype(str)\n",
    "    def has_exact(cell):\n",
    "        raw = str(cell)\n",
    "        for sep in [\";\", \",\", \"/\", \"\\\\\", \"|\", \"-\", \" \"]:\n",
    "            raw = raw.replace(sep, \"|\")\n",
    "        toks = [t.strip() for t in raw.split(\"|\") if t.strip()]\n",
    "        return str(hat_no) in toks\n",
    "    mask = s.apply(has_exact) | s.str.contains(fr\"\\b{hat_no}\\b\", regex=True, na=False)\n",
    "    out = df[mask].copy()\n",
    "    out[\"ENLEM\"]  = pd.to_numeric(out[\"ENLEM\"], errors=\"coerce\")\n",
    "    out[\"BOYLAM\"] = pd.to_numeric(out[\"BOYLAM\"], errors=\"coerce\")\n",
    "    return out.dropna(subset=[\"ENLEM\",\"BOYLAM\"]).drop_duplicates(subset=[\"DURAK_ID\"]).reset_index(drop=True)\n",
    "\n",
    "# 3) En yakın durak (vektörize haversine)\n",
    "def nearest(df, lat, lon):\n",
    "    lat1, lon1 = np.radians([lat, lon])\n",
    "    xy = np.radians(df[['ENLEM','BOYLAM']].values)\n",
    "    dlat, dlon = xy[:,0]-lat1, xy[:,1]-lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(xy[:,0])*np.sin(dlon/2)**2\n",
    "    dist_m = (6371*2*np.arcsin(np.sqrt(a)))*1000\n",
    "    i = int(dist_m.argmin())\n",
    "    return df.iloc[i], float(dist_m[i])\n",
    "\n",
    "# 4) Tabloyu oluşturalım\n",
    "dsets = {h: stops_for(h, duraklar) for h in hats}\n",
    "rows=[]\n",
    "for p in known_points:\n",
    "    for h in hats:\n",
    "        df = dsets[h]\n",
    "        if df.empty:\n",
    "            rows.append({\"NOKTA\":p[\"name\"], \"HAT\":h, \"ESIK_m\":radius_m,\n",
    "                         \"GECIYOR_MU\":False, \"MESAFE_m\":None, \"EN_YAKIN_DURAK\":None, \"DURAK_ID\":None})\n",
    "            continue\n",
    "        near, d = nearest(df, p[\"lat\"], p[\"lon\"])\n",
    "        rows.append({\"NOKTA\":p[\"name\"], \"HAT\":h, \"ESIK_m\":radius_m,\n",
    "                     \"GECIYOR_MU\": d<=radius_m, \"MESAFE_m\":round(d,1),\n",
    "                     \"EN_YAKIN_DURAK\": near[\"DURAK_ADI\"], \"DURAK_ID\": near[\"DURAK_ID\"]})\n",
    "\n",
    "res = pd.DataFrame(rows).sort_values([\"NOKTA\",\"HAT\"]).reset_index(drop=True)\n",
    "print(res.to_string(index=False))\n",
    "\n",
    "\n",
    "for p in known_points:\n",
    "    passed = [str(h) for h in hats if res[(res[\"NOKTA\"]==p[\"name\"]) & (res[\"HAT\"]==h)][\"GECIYOR_MU\"].iloc[0]]\n",
    "    print(f\"- {p['name']}: ≤{radius_m} m içinde geçen hatlar → {', '.join(passed) if passed else 'yok'}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0a919-a17d-40b2-aee9-531c4ea07c35",
   "metadata": {},
   "source": [
    "### En Çok Sefer Yapan Hatlar\n",
    "\n",
    "Bu bölümde `long` tablosundan hareketle (her satır = 1 sefer), hat bazında **toplam sefer sayısını** hesaplayıp sıralıyorum. \n",
    "Önce genel sıralama, sonra gün tipine göre (Hafta içi / Cumartesi / Pazar) ayrı  grafiklerini oluşturalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41578ee6-f3aa-4759-91e8-c63548e75840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# long hazır değilse hızlıca üret (hareket değişkeni varsa)\n",
    "if 'long' not in globals():\n",
    "    assert 'hareket' in globals(), \"Önce veri hazırlık hücrelerini çalıştır (hareket/long oluşturulmalı).\"\n",
    "    tarife_map = {1: \"Hafta içi\", 2: \"Cumartesi\", 3: \"Pazar\"}\n",
    "    hareket[\"HAT_NO\"] = pd.to_numeric(hareket[\"HAT_NO\"], errors=\"coerce\")\n",
    "    hareket[\"GUN\"] = hareket[\"TARIFE_ID\"].map(tarife_map)\n",
    "    long = pd.melt(hareket, id_vars=[\"HAT_NO\",\"TARIFE_ID\",\"GUN\"],\n",
    "                   value_vars=[\"GIDIS_SAATI\",\"DONUS_SAATI\"],\n",
    "                   var_name=\"YON\", value_name=\"SAAT_STR\")\n",
    "    long = long[long[\"SAAT_STR\"].str.match(r\"^\\d{2}:\\d{2}$\", na=False)].copy()\n",
    "    long[\"SAAT\"] = pd.to_datetime(long[\"SAAT_STR\"], format=\"%H:%M\").dt.hour\n",
    "\n",
    "# ---- Genel sıralama\n",
    "overall = (long.groupby(\"HAT_NO\").size()\n",
    "           .rename(\"SEFER_SAYISI\").reset_index()\n",
    "           .sort_values(\"SEFER_SAYISI\", ascending=False).reset_index(drop=True))\n",
    "overall[\"SIRA\"] = np.arange(1, len(overall)+1)\n",
    "display(overall.head(30))  # Top 30 tablo\n",
    "\n",
    "# CSV olarak kaydet\n",
    "out_dir = Path(\".\")\n",
    "overall_csv = out_dir / \"top_hats_overall.csv\"\n",
    "overall.to_csv(overall_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Kaydedildi:\", overall_csv.as_posix())\n",
    "\n",
    "# ---- Top 10 bar (tek grafik)\n",
    "top10 = overall.head(10).iloc[::-1]  # y ekseninde büyükten küçüğe\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(top10[\"HAT_NO\"].astype(str), top10[\"SEFER_SAYISI\"])\n",
    "plt.title(\"En Çok Sefer Yapan 10 Hat (Toplam)\")\n",
    "plt.xlabel(\"Sefer Sayısı (adet)\"); plt.ylabel(\"Hat No\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# İlgilendiğimiz hatlar nerede?\n",
    "for h in [757, 619]:\n",
    "    row = overall.loc[overall[\"HAT_NO\"]==h, [\"SIRA\",\"SEFER_SAYISI\"]]\n",
    "    if not row.empty:\n",
    "        print(f\"Hat {h}: Sıra {int(row.iloc[0,0])}, Sefer {int(row.iloc[0,1])}\")\n",
    "    else:\n",
    "        print(f\"Hat {h}: veri setinde yok.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a55950-58ae-477d-8b3f-c9f169ffd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gün tipine göre Top 10 (her biri ayrı grafik)\n",
    "by_day = (long.groupby([\"HAT_NO\",\"GUN\"]).size()\n",
    "          .rename(\"SEFER_SAYISI\").reset_index())\n",
    "\n",
    "for gun in [\"Hafta içi\",\"Cumartesi\",\"Pazar\"]:\n",
    "    df = (by_day[by_day[\"GUN\"]==gun]\n",
    "          .sort_values(\"SEFER_SAYISI\", ascending=False).head(10).iloc[::-1])\n",
    "    if df.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.barh(df[\"HAT_NO\"].astype(str), df[\"SEFER_SAYISI\"])\n",
    "    plt.title(f\"Top 10 Hat – {gun}\")\n",
    "    plt.xlabel(\"Sefer Sayısı (adet)\"); plt.ylabel(\"Hat No\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29215294-0bda-41ee-8b7c-e8cc3a3f06b3",
   "metadata": {},
   "source": [
    "### Sonuç\n",
    "\n",
    "Bu çalışmada 757 ve 619 hatları için veriyi hazırlayıp gün×saat düzeyinde sefer yoğunluğu çıkardım. Isı haritaları, 757’de hafta içi 06–08 bandında keskin bir sabah zirvesi olduğunu; 619’da ise yoğunluğun gün boyu daha dengeli seyrettiğini ve iki hatta da hafta sonu belirgin düşüş yaşandığını gösterdi. Durak verisi ile yaptığım konumsal doğrulamada Ulukent Aktarma Merkezi ve Bakırçay Üniversitesi için ≤300 m içinde durak tespit ettim (erişim doğrulandı). Ek olarak “en çok sefer yapanlar” sıralaması genel tabloyu destekledi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f493fb1c-4398-43ce-9f81-f6670bec8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasörler hazır: ['data', 'notebooks', 'outputs']\n"
     ]
    }
   ],
   "source": [
    "# Python ile platformdan bağımsız:\n",
    "import os\n",
    "base = \"izmir-bus-service-analysis\"\n",
    "for d in [\"notebooks\", \"outputs\", \"data\"]:\n",
    "    os.makedirs(os.path.join(base, d), exist_ok=True)\n",
    "print(\"Klasörler hazır:\", os.listdir(base))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc56ea90-360e-414f-a47c-9d8ef62da6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosyalar yazıldı.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "base = Path(\"izmir-bus-service-analysis\")\n",
    "\n",
    "(base/\"README.md\").write_text(\n",
    "\"# İzmir Otobüs Sefer Yoğunluğu – 757 & 619\\n\\n\"\n",
    "\"Bu repo, 757 ve 619 için gün×saat sefer yoğunluğu, durakların harita üzerinde gösterimi \"\n",
    "\"(≤300 m erişim doğrulaması) ve opsiyonel K-Means kümelemeyi içerir.\\n\\n\"\n",
    "\"## Veri Setleri\\n\"\n",
    "\"- eshot-otobus-hareketsaatleri.csv (ayraç `;`, TARIFE_ID→{Hafta içi, Cumartesi, Pazar} varsayımı)\\n\"\n",
    "\"- eshot-otobus-duraklari.csv (ENLEM/BOYLAM WGS84, DURAKTAN_GECEN_HATLAR token’lı)\\n\"\n",
    "\"> Not: Büyük/lisanslı verileri repo dışı tutuyorum; notebook `data/` altından okur.\\n\\n\"\n",
    "\"## Çalıştırma\\n\"\n",
    "\"```bash\\n\"\n",
    "\"python -m venv .venv && source .venv/bin/activate  # Windows: .venv\\\\Scripts\\\\activate\\n\"\n",
    "\"pip install -r requirements.txt\\n\"\n",
    "\"jupyter lab\\n\"\n",
    "\"```\\n\"\n",
    ", encoding=\"utf-8\")\n",
    "\n",
    "(base/\".gitignore\").write_text(\n",
    "\"data/*.csv\\n.venv/\\n__pycache__/\\n.ipynb_checkpoints/\\n.DS_Store\\n\", encoding=\"utf-8\")\n",
    "\n",
    "(base/\"requirements.txt\").write_text(\n",
    "\"pandas\\nnumpy\\nmatplotlib\\nfolium\\nscikit-learn\\nthreadpoolctl\\njupyterlab\\n\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Dosyalar yazıldı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65838f-301c-4b17-b90e-02657f783b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
